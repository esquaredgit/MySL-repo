{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a69b3f-8101-4e9b-8fe2-c9b98ca73538",
   "metadata": {},
   "source": [
    "# MySL (My Science Liaison): a Literature-Informed, LLM-Powered Medical Science Liaison\n",
    "\n",
    "## Context & Problem Statement\n",
    "Medical Science Liasons, or MSLs for short, are industry-facing medical scientists that have a wide variety of responsibilities in the pharmaceutical industry. One of these responsibilities is to maintain relationships with physicians, pharmacists, and other key opinion leaders (KOLs) in the medical community. These relationships are usually maintained through frequent interactions initiated by the MSL, in which the MSL answers detailed scientific questions about their company's products. Through these interactions, the MSL ensures that members of the medical community have up-to-date, scientifically accurate information. \n",
    "\n",
    "Though MSLs usually hold advanced degrees in the life sciences and are more than capable of fulfilling the role of science communicator, the amount of effort required to be constantly available to their clients is disproportionately high. MSLs spend a significant portion of their time chasing interaction quotas and traveling to meet KOLs in person. Though not all of this travel is Q&A-related, even a small reduction in workload could allow MSLs to better allocate their time and efforts to more pressing problems, especially in a world where relevant medical information is readily available online. \n",
    "\n",
    "## Solution\n",
    "This problem has a convenient and complementary solution in the form of Large Language Models (LLMs). These transformer-based models are trained on a diverse and substantial corpus of data, and are engineered to answer queries in natural language. This feature of LLMs resolves one of the primary sources of friction keeping doctors and pharmacists from simply searching for technical drug information themselves. Though pharmaceutical companies digitally publish all of the scientific information needed to answer frequently asked questions, their customers have trouble accessing it in a reliable and digestible format. For this reason, an MSL must dedicate a significant amount of time to reviewing publically available drug information and relating it to their clients. \n",
    "\n",
    "My Science Liaison (MySL) is an LLM-powered chat application that aims to fill an MSL's role as science communicator. By using Retrieval Augmented Generation (RAG) pipelines to reference the most up-to-date info available, MySL is built to be scientifically informed and reliable. Instead of spending hours reviewing new literature, getting MySL up to speed on the latest product information takes as long as uploading/accessing a simple text document. To ensure interpretability, MySL's responses will even reference the source documents it used to create an answer, allowing the user to easily verify the information themselves in the case of model hallucinations. Future versions will also incorporate pretraining on scientific documents to tailor MySL's speech patterns and built-in knowledge to medical information. \n",
    "\n",
    "## Methods\n",
    "MySL was written in Python, making extensive use of the Langchain package to facilitate the model's RAG pipelines. Though there is room to experiment with different LLMs, OpenAI's GPT-4o-mini was used in this particular implementation of the model simply because of its fast inference time and low cost. For commercial applications where cost is less of an issue and external compute resources are available, testing the use of larger models like GPT-4 from OpenAI and Claude-3.5 Sonnet from Anthropic is recommended. \n",
    "\n",
    "MySL's knowledge base was compiled from a variety of different publicly available sources, and was curated to have specialized knowledge of some of the most popular ulcerative colitis (UC) medications. UC was chosen simply because I have a friend that currently works as an MSL in the UC indication, so I'll have someone to test the project out on. Pharmaceutical companies make prescribing data for their products available online, so for this implementation, documents were pulled from the healthcare provider (HCP) websites of Eli Lilly (mirikizumab/Omvoh), Janssen (ustekinumab/Stelara), and Takeda (vedolizumab/Entyvio).\n",
    "\n",
    "## Miscellaneous Notes\n",
    "This project was made as a minimum viable product, and as such does not contain many of the available features and improvements it should have before being used in production. Future improvements include, but are not limited to:  \n",
    "* pretraining on relevant medical and pharmaceutical knowledge\n",
    "* an intuitive user interface that shows chat history and takes users to the specific places in its source documents that references were made to\n",
    "* testing on various RAG pipeline hyperparameters such as chunk size, similarity functions, query modification/translation methods, and more\n",
    "* inclusion of conversation history in the model's context\n",
    "\n",
    "## Model Implementation:\n",
    "First, the necessary environment variables are imported. The specific LLM powering the application is also initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a0ca9e-eba7-4fd3-be4d-695b747455bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4e2b2-e68d-4998-a7bc-5c9f8e96fe97",
   "metadata": {},
   "source": [
    "Next, the data is loaded in and cleaned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf787ea-deb7-4386-b468-fbe7652a900d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://uspl.lilly.com/omvoh/omvoh.html#pi', 'title': 'Omvoh Prescribing Information', 'language': 'en', 'page': 0}\n",
      "\n",
      "\n",
      "These highlights do not include all the information needed to use OMVOH safely and effectively. See full prescribing information for OMVOH. OMVOH (mirikizumab-mrkz) injection, for intravenous or subcutaneous use Initial U.S. Approval: 2023 OMVOH- mirikizumab-mrkz injection, solution Eli Lilly and Company ---------- HIGHLIGHTS OF PRESCRIBING INFORMATION These highlights do not include all the information needed to use OMVOH safely and effectively. See full prescribing information for OMVOH. OMVOH\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "def clean_text_string(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    remove_whitespace = [line.strip() for line in lines if line.strip()]\n",
    "    join_lines = '\\n'.join(remove_whitespace)\n",
    "    # Remove excessive spaces\n",
    "    clean_text = ' '.join(join_lines.split())\n",
    "    return clean_text\n",
    "\n",
    "# Load web data\n",
    "data_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://uspl.lilly.com/omvoh/omvoh.html#pi\",),\n",
    ")\n",
    "docs = data_loader.load()\n",
    "\n",
    "# Add relevant metadata\n",
    "docs[0].metadata[\"title\"] = \"Omvoh Prescribing Information\"\n",
    "docs[0].metadata[\"page\"] = 0\n",
    "\n",
    "# Clean web data\n",
    "for doc in docs:\n",
    "    doc.page_content = clean_text_string(doc.page_content)\n",
    "\n",
    "# Display results\n",
    "print(f\"{docs[0].metadata}\\n\\n\")\n",
    "print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc57319-c9a8-4c0a-9f7f-f6a93e7f5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 PDFS LOADED; 32 TOTAL PAGES: \n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 0, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 1, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 2, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 3, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 4, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 5, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 6, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 7, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 8, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 9, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 10, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 11, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/ENTYVIOPI.pdf', 'page': 12, 'title': 'Entyvio Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 0, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 1, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 2, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 3, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 4, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 5, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 6, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 7, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 8, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 9, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 10, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 11, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 12, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 13, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 14, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 15, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 16, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 17, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n",
      "{'source': 'data/STELARA-pi.pdf', 'page': 18, 'title': 'Stelara Prescribing Information', 'language': 'en'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Loading PDF files\n",
    "\n",
    "pdf_paths = {\"Entyvio Prescribing Information\": \"data/ENTYVIOPI.pdf\", \"Stelara Prescribing Information\": \"data/STELARA-pi.pdf\"}\n",
    "loaded_pdfs = []\n",
    "\n",
    "for pdf in pdf_paths.keys():\n",
    "    loader = PyPDFLoader(pdf_paths[pdf])\n",
    "    async for page in loader.alazy_load():\n",
    "        page.metadata[\"title\"] = pdf\n",
    "        page.metadata[\"language\"] = \"en\"\n",
    "        loaded_pdfs.append(page)\n",
    "\n",
    "print(f\"{len(pdf_paths.keys())} PDFS LOADED; {len(loaded_pdfs)} TOTAL PAGES: \\n\\n\")\n",
    "for pdf in loaded_pdfs:\n",
    "    print(f\"{pdf.metadata}\\n\\n\")\n",
    "\n",
    "# Consolidate all docs in existing docs var\n",
    "docs.extend(loaded_pdfs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa82868-7042-4796-add8-135b0de9fd0e",
   "metadata": {},
   "source": [
    "After that, the documents are split using recursive splitting so as to conserve token space in calls to the LLM. They are then indexed in a vector store: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eceb4859-75b5-413f-be00-3406d2a7073e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'996 characters'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "str(len(splits[0].page_content))+\" characters\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670cb20-c0a3-4973-bdd8-d719e0230829",
   "metadata": {},
   "source": [
    "A document retriever is made from the vector store. A system prompt template is then written, and a helper function is created to format the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d076c35b-53a0-4e2b-86b4-34d1c12d3132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in medical pharmacology. You work as a Medical Science Liaison, and your job is to answer technical questions from other medical professionals while being as scientifically accurate as possible. You specialize in the Ulcerative Colitis indication, and you focus on three competing medications: Omvoh, Stelara, and Entyvio. Use the following pieces of retrieved context to answer any questions you receive. If you don't know the answer, say that you don't know. Politely REFUSE TO ANSWER any questions unrelated to pharmacology and medicine.\n",
      "\n",
      "CONTEXT:\n",
      "{context}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert in medical pharmacology. You work as a \"\n",
    "    \"Medical Science Liaison, and your job is to answer technical \"\n",
    "    \"questions from other medical professionals while being as\"\n",
    "    \" scientifically accurate as possible. You specialize in the Ulcerative\"\n",
    "    \" Colitis indication, and you focus on three competing medications:\"\n",
    "    \" Omvoh, Stelara, and Entyvio. Use the following pieces \"\n",
    "    \"of retrieved context to answer any questions you receive.\"\n",
    "    \" If you don't know the answer, say that you don't know. \"\n",
    "    \"Politely REFUSE TO ANSWER any questions\"\n",
    "    \" unrelated to pharmacology and medicine.\"\n",
    "    \"\\n\\n\"\n",
    "    \"CONTEXT:\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c413d-6b90-45ae-abcd-a3dc0d3ca887",
   "metadata": {},
   "source": [
    "The RAG chain is then defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ad901a-7565-4ad2-9f6f-4ba026a97e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, qa_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f7875-35f9-48e8-8257-4ca69f6b53ad",
   "metadata": {},
   "source": [
    "Finally, the user's prompt is entered and processed: \n",
    "\n",
    "\"What are some alternatives to Omvoh for treating Ulcerative Colitis, and how do they compare in terms of dosage?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f390919f-7a6d-4bd7-b458-af37e3cf74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are some alternatives to Omvoh for treating Ulcerative Colitis, and how do they compare in terms of dosage?\"\n",
    "output = rag_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b60ba6-3b08-447e-bc02-684356c59e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE:\n",
      "Some alternatives to OMVOH for the treatment of moderately to severely active ulcerative colitis include Stelara (ustekinumab) and Entyvio (vedolizumab).\n",
      "\n",
      "1. **Stelara (ustekinumab)**:\n",
      "   - **Dosage**: The initial dose is administered as a weight-based intravenous infusion (typically 260 mg for those weighing up to 100 kg, 390 mg for those weighing 100 kg to 120 kg, and 520 mg for those over 120 kg) followed by a subcutaneous injection at Week 8 and then every 12 weeks thereafter.\n",
      "\n",
      "2. **Entyvio (vedolizumab)**:\n",
      "   - **Dosage**: The initial dose is given as an intravenous infusion of 300 mg at Weeks 0, 2, and 6, followed by maintenance infusions every 8 weeks.\n",
      "\n",
      "These medications have different mechanisms of action and dosing schedules, which may influence their selection based on individual patient needs and response to therapy. Always consult with a healthcare provider for personalized treatment plans.\n"
     ]
    }
   ],
   "source": [
    "print(\"RESPONSE:\\n\" + output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d93159c-3d89-4c1c-9241-7bbede8ad4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT: \n",
      "trials are conducted under widely varying conditions, adverse reaction rates observed in the clinical trials of a drug cannot be directly compared to rates in the clinical trials of another drug and may not reflect the rates observed in practice. OMVOH was studied up to 12 weeks in subjects with moderately to severely active ulcerative colitis in a randomized, double-blind, placebo-controlled induction study (UC-1). In subjects who responded to induction therapy in UC-1, long term safety up to 52 weeks was evaluated in a randomized, double-blind, placebo-controlled maintenance study (UC-2) and a long-term extension study [see Clinical Studies (14)]. In the induction study (UC-1), 1279 subjects were enrolled of whom 958 received OMVOH 300 mg administered as an intravenous infusion at Weeks 0, 4, and 8. In the maintenance study (UC-2), 581 subjects were enrolled of whom 389 received OMVOH 200 mg administered as a subcutaneous injection every 4 weeks. Table 1 summarizes the adverse\n",
      "\n",
      "PRECAUTIONS 5.1 Hypersensitivity Reactions 5.2 Infections 5.3 Tuberculosis 5.4 Hepatotoxicity 5.5 Immunizations 6 ADVERSE REACTIONS 6.1 Clinical Trials Experience 8 USE IN SPECIFIC POPULATIONS 8.1 Pregnancy 8.2 Lactation 8.4 Pediatric Use 8.5 Geriatric Use 11 DESCRIPTION 12 CLINICAL PHARMACOLOGY 12.1 Mechanism of Action 12.2 Pharmacodynamics 12.3 Pharmacokinetics 12.6 Immunogenicity 13 NONCLINICAL TOXICOLOGY 13.1 Carcinogenesis, Mutagenesis, Impairment of Fertility 14 CLINICAL STUDIES 16 HOW SUPPLIED/STORAGE AND HANDLING 17 PATIENT COUNSELING INFORMATION * Sections or subsections omitted from the full prescribing information are not listed. FULL PRESCRIBING INFORMATION 1 INDICATIONS AND USAGE OMVOH is indicated for the treatment of moderately to severely active ulcerative colitis in adults. 2 DOSAGE AND ADMINISTRATION 2.1 Recommended Evaluations and Immunizations Prior to Treatment Initiation Evaluate patients for tuberculosis (TB) infection prior to initiating treatment with OMVOH\n",
      "\n",
      "These highlights do not include all the information needed to use OMVOH safely and effectively. See full prescribing information for OMVOH. OMVOH (mirikizumab-mrkz) injection, for intravenous or subcutaneous use Initial U.S. Approval: 2023 OMVOH- mirikizumab-mrkz injection, solution Eli Lilly and Company ---------- HIGHLIGHTS OF PRESCRIBING INFORMATION These highlights do not include all the information needed to use OMVOH safely and effectively. See full prescribing information for OMVOH. OMVOH (mirikizumab-mrkz) injection, for intravenous or subcutaneous use Initial U.S. Approval: 2023 RECENT MAJOR CHANGES Dosage and Administration (2.4) 04/2024 INDICATIONS AND USAGE OMVOHTM is an interleukin-23 antagonist indicated for the treatment of moderately to severely active ulcerative colitis in adults (1) DOSAGE AND ADMINISTRATION Prior to Treatment Initiation Evaluate patients for tuberculosis (TB) infection. (2.1, 5.3) Obtain liver enzymes and bilirubin levels. (2.1, 5.4) Complete all\n",
      "\n",
      "begin treatment with OMVOH. Your healthcare provider should watch you closely for signs and symptoms of TB while you are being treated with OMVOH and after treatment. Before starting OMVOH, tell your healthcare provider if you think you have an infection or have symptoms of an infection such as: fever, sweating, or chills muscle aches and pain cough or shortness of breath blood in your mucus (phlegm) flu-like symptoms headache warm, red, or painful skin or sores on your body diarrhea or stomach pain weight loss nausea or vomiting pain during urination After starting OMVOH, tell your healthcare provider right away if you have any symptoms of an infection. Liver problems. OMVOH may cause liver problems. Your healthcare provider will do blood tests to check your liver enzyme and bilirubin levels before treatment, for at least 24 weeks during treatment, and possibly after treatment with OMVOH. Your healthcare provider may hold or stop your treatment if needed. Tell your healthcare\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(\"CONTEXT: \\n\" + format_docs(output[\"context\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5fbc4-5033-4471-a9ab-b53464857611",
   "metadata": {},
   "source": [
    "## Discussion of Preliminary Results\n",
    "\n",
    "There is a lot to observe from our first round of results. After fact-checking the response, everything MySL recommended about dosage information seems to be correct for all medications. All of the facts, from dosage amounts, frequencies, and administration methods can be verified in the indexed context documents. Mission success, right?\n",
    "\n",
    "Sort of.\n",
    "\n",
    "If we take a closer look at the retrieved chunks from the RAG pipeline, we can see that only Omvoh was mentioned in the context. This presumably means that the model is relying on its training information for any dosage advice for non-Omvoh medications. Although this response ended up being correct, this is probably only because the necessary information made it into the model's knowledge cutoff. In other words, we got lucky. For future queries, we want to ensure that any information necessary to answer the question is present in the context, or has been verifiably included in the model's training. LLMs are prone to \"hallucinations\" when they are missing information, and by relying on a black box of knowledge to make up for lack of context, we are increasing the probability that MySL gives false information to its users. \n",
    "\n",
    "## Possible Solutions to Missing Context\n",
    "\n",
    "To decrease our chances of model hallucinations, we can take a few different approaches:\n",
    "1. Make sure our model is trained on comprehensive and up-to-date info that covers any question our users might ask\n",
    "2. Make sure this info is at least indexed in the RAG vector store\n",
    "3. Assuming this info is present in our vector store, we can improve our RAG pipeline's retrieval methods to get a more relevant sampling of information from the index\n",
    "4. Make sure our model does not answer anything it doesn't have the references for\n",
    "5. Use web search as a fallback\n",
    "\n",
    "Given that this implementation is an MVP, and that getting \"comprehensive and up-to-date info that covers any question our users might ask\" would take a lot of user surveying/data monitoring, we'll settle for improving the retrieval methods for the time being.\n",
    "\n",
    "## Analysis of Retrieval Methods\n",
    "\n",
    "Our current implementation is not taking advantage of query translation/modification steps to enhance its results. We are using maximal marginal relevance (MMR) to measure document similarity to our user's query, so we should be getting a diverse mix of context chunks from our library. The only issue is that certain questions, like the example asked above, need more than just a good similarity function to get the right context. \n",
    "\n",
    "\"What are some alternatives to Omvoh, and how do they compare in terms of dosage?\" is an interesting question because although it explicitly asks the model to think of other medications besides Omvoh, Omvoh is the only medication mentioned in the query. The LLM itself understands this, but our context retrieval is a bit more naive. When running similarity calculations between a query and the indexed documents, only documents containing words similar to the query will be returned. Given that Omvoh is the only medication mentioned in the query, only context chunks containing \"Omvoh\" are likely to be returned. We can see from the context returned in the results above that this is exactly what happened. The only reason that Entyvio and Stelara were mentioned in the response was because they were explained to be similar medications in the system prompt. \n",
    "\n",
    "Even though Omvoh is the only medication mentioned in the query, we as humans know that to answer the question correctly, we need context on other medications besides Omvoh. We would begin to attack this question by asking \"what are the known alternatives to Omvoh?\" Then, we would ask \"what are the dosage recommendations for each alternative, and how do they compare to that of Omvoh?\"  We can mimic this multi-query approach to answering the question by using Query Translation to augment our retrieval.\n",
    "\n",
    "## Implementation of Recursive Query Decomposition\n",
    "\n",
    "Query Translation is a general term for taking the user's query and modifying/augmenting it somehow to improve retrieval results. One commonly used Query Translation method is called Query Decomposition. This approach takes the original query given to the model, instructs an LLM to create N sub-questions that would lead to an answer for the user's query, then feeds those to the retrieval system for addtional context. We are going to recursively feed in each sub-question so that each one has the Q&A pairs of the sub-questions before it as context. We will then feed the results of this process, along with the user's original query, to construct an adequate answer (this time, with better context). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7b81b-16bf-47cb-8d94-a58921555833",
   "metadata": {},
   "source": [
    "We'll start by creating 3 sub-questions from our original query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "092055a6-ffe9-45f8-8929-a9b63f943afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What are the most common alternatives to Omvoh for treating Ulcerative Colitis?\n",
      "2. How do the dosages of alternative medications for Ulcerative Colitis compare to Omvoh?\n",
      "3. What are the side effects of alternative treatments for Ulcerative Colitis compared to Omvoh?\n"
     ]
    }
   ],
   "source": [
    "# Creating decomposition template\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":query})\n",
    "\n",
    "for question in questions: \n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b5c3a-8950-491f-9bbc-ab4effb43628",
   "metadata": {},
   "source": [
    "Next, we'll set up some additional infrastructure to handle our chain-of-thought reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad688084-d22e-4515-acc8-ee4e0df67f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Question: 1. What are the most common alternatives to Omvoh for treating Ulcerative Colitis?\n",
      "Answer: The most common alternatives to Omvoh for treating Ulcerative Colitis include Stelara (ustekinumab) and Entyvio (vedolizumab). Both of these medications are indicated for the treatment of moderately to severely active ulcerative colitis and have been studied in clinical trials for their efficacy and safety profiles. If you have specific questions about their mechanisms of action, dosing, or clinical trial data, feel free to ask!\n",
      "---\n",
      "Question: 2. How do the dosages of alternative medications for Ulcerative Colitis compare to Omvoh?\n",
      "Answer: The dosages of alternative medications for Ulcerative Colitis, such as Stelara (ustekinumab) and Entyvio (vedolizumab), differ from those of Omvoh (mirikizumab). \n",
      "\n",
      "For Omvoh, the recommended dosing regimen involves an intravenous infusion of 300 mg at Weeks 0, 4, and 8 for induction, followed by a subcutaneous injection of 200 mg every 4 weeks for maintenance. \n",
      "\n",
      "In contrast, Stelara is typically administered as an initial intravenous dose of 260 mg for adults, followed by a subcutaneous injection of 90 mg every 8 weeks after the initial dose. Entyvio is administered as an intravenous infusion of 300 mg at Weeks 0, 2, and 6, followed by maintenance infusions every 8 weeks.\n",
      "\n",
      "These differences in dosing schedules and routes of administration reflect the unique pharmacokinetics and mechanisms of action of each medication. If you have further questions about specific dosing or clinical considerations, feel free to ask!\n",
      "---\n",
      "Question: 3. What are the side effects of alternative treatments for Ulcerative Colitis compared to Omvoh?\n",
      "Answer: I don't have specific information on the side effects of alternative treatments for Ulcerative Colitis compared to Omvoh. However, I can tell you that Omvoh (mirikizumab) has been associated with potential side effects such as hypersensitivity reactions, infections, hepatotoxicity, and the need for monitoring liver function. For detailed information on the side effects of Stelara (ustekinumab) and Entyvio (vedolizumab), I recommend consulting their respective prescribing information or clinical guidelines. If you have any other questions related to pharmacology or these medications, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Master prompt template\n",
    "template = \"\"\"\n",
    "You are an expert in medical pharmacology. \n",
    "You work as a Medical Science Liaison, and your job is to answer technical\n",
    "questions from other medical professionals while being as\n",
    "scientifically accurate as possible. You specialize in the Ulcerative\n",
    "Colitis indication, and you focus on three competing medications:\n",
    "Omvoh, Stelara, and Entyvio. Use the following pieces\n",
    "of retrieved context to answer any questions you receive.\n",
    "If you don't know the answer, say that you don't know.\n",
    "Politely REFUSE TO ANSWER any questions\n",
    "unrelated to pharmacology and medicine.\n",
    "\\n\\n\n",
    "\n",
    "Here's the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for question in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":question,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(question,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "\n",
    "print(q_a_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b23af3-9e9a-4862-8854-c482b888b43c",
   "metadata": {},
   "source": [
    "Now that we have Q&A pairs made for each of our sub-questions, we will send this to our retriever to get context, then pass the context and the Q&A pairs to our model to answer the original question: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc0f9e6b-cef4-4e3f-bc1b-20cdd260db79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      "The most common alternatives to Omvoh for treating Ulcerative Colitis include Stelara (ustekinumab) and Entyvio (vedolizumab). \n",
      "\n",
      "In terms of dosage:\n",
      "\n",
      "- **Omvoh (mirikizumab)**: The recommended dosing regimen involves an intravenous infusion of 300 mg at Weeks 0, 4, and 8 for induction, followed by a subcutaneous injection of 200 mg every 4 weeks for maintenance.\n",
      "\n",
      "- **Stelara (ustekinumab)**: It is typically administered as an initial intravenous dose of 260 mg for adults, followed by a subcutaneous injection of 90 mg every 8 weeks after the initial dose.\n",
      "\n",
      "- **Entyvio (vedolizumab)**: This medication is administered as an intravenous infusion of 300 mg at Weeks 0, 2, and 6, followed by maintenance infusions every 8 weeks.\n",
      "\n",
      "These differences in dosing schedules and routes of administration reflect the unique pharmacokinetics and mechanisms of action of each medication. If you have further questions about specific dosing or clinical considerations, feel free to ask!\n",
      "\n",
      "\n",
      "CONTEXT (aside from the Q&A pairs):\n",
      "\n",
      "levels before treatment, for at least 24 weeks during treatment, and possibly after treatment with OMVOH. Your healthcare provider may hold or stop your treatment if needed. Tell your healthcare provider right away if you develop any signs and symptoms of liver problems, including: unexplained rash nausea vomiting stomach-area (abdominal) pain feeling tired loss of appetite yellowing of the skin or the whites of your eyes dark urine What is OMVOH? OMVOH is a prescription medicine used in adults with moderately to severely active ulcerative colitis. It is not known if OMVOH is safe and effective in children. Do not use OMVOH if you: are allergic to mirikizumab-mrkz or any of the ingredients in OMVOH. See the end of this Medication Guide for a complete list of ingredients in OMVOH. Before you use OMVOH, tell your healthcare provider about all your medical conditions, including if you: have any of the conditions or symptoms listed in the section \"What is the most important information I\n",
      "\n",
      "\n",
      "Exposure at Steady State in Subjects with Ulcerative Colitis a OMVOH 300 mg as an intravenous infusion over at least 30 minutes at Weeks 0, 4, and 8. b OMVOH 200 mg as a subcutaneous injection at Week 12 and every 4 weeks thereafter for up to an additional 40 weeks. c AUCtau, ss = area under the concentration-versus-time curve over one dosing interval at steady state; Cmax, ss = maximum concentration at steady state; Ctrough, ss = concentration at the end of the dosing interval at steady state; CV = geometric coefficient of variation. OMVOH300 mg Intravenous Infusiona Geometric mean (CV%) OMVOH200 mg Subcutaneous Injectionb Geometric mean (CV%) Cmax, ss (µg/mL)c 99.7 (22.7%) 10.1 (52.1%) AUCtau, ss (µg*day/mL)c 538 (34.4%) 160 (57.6%) Ctrough, ss (µg/mL)c 2.75 (101%) 1.70 (83.3%) Absorption Following subcutaneous dosing of OMVOH, median (range) Tmax was 5 (3.08 to 6.75) days post dose and geometric mean (CV%) absolute bioavailability was 44% (34%). Injection site location (abdomen,\n",
      "\n",
      "\n",
      "to vedolizumab. Two of the 7 patients with Crohn’s disease and none of the 6 patients with \n",
      "ulcerative colitis who had positive anti-vedolizumab antibodies achieved clinical remission \n",
      "at Week 52. There is insufficient data to assess the effect of anti-drug antibodies on \n",
      "pharmacokinetics, effectiveness, and safety of ENTYVIO in the SC UC and SC CD trials.\n",
      "13 NONCLINICAL TOXICOLOGY\n",
      "13.1 Carcinogenesis, Mutagenesis, Impairment of Fertility\n",
      "Long-term studies in animals have not been performed to evaluate the carcinogenic potential \n",
      "of vedolizumab. Studies to evaluate the possible impairment of fertility or mutagenic potential \n",
      "of vedolizumab have not been performed.\n",
      "14 CLINICAL STUDIES\n",
      "14.1 Clinical Studies in Ulcerative Colitis\n",
      "Intravenous Administration\n",
      "The safety and efficacy of intravenous ENTYVIO were evaluated in two randomized, double-\n",
      "blind, placebo-controlled trials (UC Trials I and II) in adult patients with moderately to\n",
      "\n",
      "\n",
      "were eligible to receive a 90 mg subcutaneous injection of STELARA® upon entry \n",
      "into trial CD-3. Of these patients, 102/219 (47%) achieved clinical response eight \n",
      "weeks later and were followed for the duration of the trial.\n",
      "14.5 Ulcerative Colitis\n",
      "STELARA® was evaluated in two randomized, double-blind, placebo-controlled \n",
      "clinical trials [UC-1 and UC-2 (NCT02407236)] in adult patients with moderately to \n",
      "severely active ulcerative colitis who had an inadequate response to or failed to \n",
      "tolerate a biologic (i.e., TNF blocker and/or vedolizumab), corticosteroids, and/or \n",
      "6-MP or AZA therapy. The 8-week intravenous induction trial (UC-1) was followed \n",
      "by the 44-week subcutaneous randomized withdrawal maintenance trial (UC-2) for \n",
      "a total of 52 weeks of therapy.\n",
      "Disease assessment was based on the Mayo score, which ranged from 0 to 12 \n",
      "and has four subscores that were each scored from 0 (normal) to 3 (most severe):\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an expert in medical pharmacology. \n",
    "You work as a Medical Science Liaison, and your job is to answer technical\n",
    "questions from other medical professionals while being as\n",
    "scientifically accurate as possible. You specialize in the Ulcerative\n",
    "Colitis indication, and you focus on three competing medications:\n",
    "Omvoh, Stelara, and Entyvio. Use the following pieces\n",
    "of retrieved context to answer any questions you receive.\n",
    "If you don't know the answer, say that you don't know.\n",
    "Politely REFUSE TO ANSWER any questions\n",
    "unrelated to pharmacology and medicine.\n",
    "\\n\\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {{context}} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],  # input query\n",
    "        \"context\": lambda x: format_docs(x[\"context\"]),  # context\n",
    "    }\n",
    "    | prompt  # format query and context into prompt\n",
    "    | llm  # generate response\n",
    "    | StrOutputParser()  # coerce to string\n",
    ")\n",
    "\n",
    "retrieved_docs = (lambda q: f\"{q}\\n\\n{q_a_pairs}\") | retriever\n",
    "\n",
    "chain = RunnablePassthrough.assign(context=retrieved_docs).assign(answer=rag_chain_from_docs)\n",
    "\n",
    "results = chain.invoke({\"input\": query})\n",
    "\n",
    "print(f\"ANSWER:\\n{results['answer']}\\n\\n\")\n",
    "print(\"CONTEXT (aside from the Q&A pairs):\\n\")\n",
    "for doc in results['context']:\n",
    "    print(doc.page_content+\"\\n\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c00140-d685-467a-a5a6-649a961275fb",
   "metadata": {},
   "source": [
    "## Final Observations (for now)\n",
    "Taking a look at the final results from our recursive decomposition implementation, it seems we have succeeded in making our RAG pipeline a bit less naive. Chunks from all relevant documents appear in the context, though not all relevant info was present in the chunks. Overall, however, this is a substantial improvement over the original implementation. Next steps for improving the model would involve experimenting with chunk size and the number of chunks to retrieve from the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578aa6d-9bf3-42f6-a620-07378a5bd8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
